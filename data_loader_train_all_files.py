import os
import numpy as np
import pickle
from sklearn.preprocessing import StandardScaler


def load_and_split_train_dataset(file_path, context_length=96, scale=True):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ï¼ˆæ­£å¸¸ï¼‰ã‚’èª­ã¿è¾¼ã¿ã€context_lengthã”ã¨ã«åˆ†å‰²ã€‚
    ç•°å¸¸ã¯ãªã„ãŸã‚ã€å…¨ã¦ã® anomaly_intervals ã¯ [[]] ã«è¨­å®šã€‚
    """
    file_name = os.path.basename(file_path)
    fields = file_name.split('_')
    meta_data = {
        'name': '_'.join(fields[:4]),
        'train_end': int(fields[4]),
    }

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(file_path) as f:
        Y = f.readlines()
        if len(Y) == 1:
            Y = Y[0].strip()
            Y = np.array([eval(y) for y in Y.split(" ") if len(y) > 1]).reshape((1, -1))
        elif len(Y) > 1:
            Y = np.array([eval(y.strip()) for y in Y]).reshape((1, -1))

    Y = Y.reshape(-1, 1)
    Y_train = Y[:meta_data['train_end']]

    if scale:
        scaler = StandardScaler()
        scaler.fit(Y_train)
        Y_train = scaler.transform(Y_train)

    length = Y_train.shape[0]
    num_chunks = length // context_length

    all_segments = []
    all_anomalies = []

    for i in range(num_chunks):
        start_idx = i * context_length
        end_idx = start_idx + context_length
        if end_idx > length:
            break

        segment = Y_train[start_idx:end_idx].copy()
        all_segments.append(segment)
        all_anomalies.append([[]])  # ç•°å¸¸ãªã—ï¼ˆæ­£å¸¸åŒºé–“ï¼‰

    return all_segments, all_anomalies


def save_dataset_as_pickle(output_dir, series_list, anomaly_list):
    os.makedirs(output_dir, exist_ok=True)
    data_dict = {
        'series': series_list,
        'anom': anomaly_list
    }
    output_path = os.path.join(output_dir, 'data.pkl')
    with open(output_path, 'wb') as f:
        pickle.dump(data_dict, f)
    print(f"âœ… ä¿å­˜: {output_path}")


def main():
    input_dir = "/Storage2/maru/datasets/UCR_Anomaly_Archive/UCR_Anomaly_FullData/"
    output_dir = "/Storage2/maru/datasets/UCR_Anomaly_Archive/AnomLLM/train/"
    context_length = 96

    files = [f for f in os.listdir(input_dir) if f.endswith('.txt')]

    print(f"ğŸ“‚ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")

    for i, file_name in enumerate(files):
        full_path = os.path.join(input_dir, file_name)

        try:
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†åˆ†å‰²
            series_list, anomaly_list = load_and_split_train_dataset(
                full_path,
                context_length=context_length,
            )

            # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®å…ˆé ­4è¦ç´ ã§ï¼‰
            save_dir_name = '_'.join(file_name.split('_')[:4])
            save_dir = os.path.join(output_dir, save_dir_name)

            save_dataset_as_pickle(save_dir, series_list, anomaly_list)

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆ{file_name}ï¼‰: {e}")


if __name__ == "__main__":
    main()

